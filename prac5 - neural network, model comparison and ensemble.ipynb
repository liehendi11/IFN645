{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 5: Neural Network\n",
    "\n",
    "### In this practical\n",
    "1. [Resuming from practical 4](#resume)\n",
    "2. [Building your first neural network model](#build)\n",
    "3. [Understanding your neural network model](#viz)\n",
    "4. [Finding optimal hyperparameters with GridSearchCV](#gridsearch)\n",
    "5. [Feature selection](#fselect)\n",
    "6. [Comparing models](#comparison)\n",
    "\n",
    "---\n",
    "\n",
    "### Important Changelog:\n",
    "* (25/07/2017) Made tutorial notes public.\n",
    "\n",
    "This practical introduces neural network mining in Python. Similar with previous practicals, our objective is to build a neural network to classify the lapsing donors based on their responses to the greeting card mailing campaign conducted by the national veterans' organisation. We will continue using **PVA97NK** dataset to predict **TARGETB**.\n",
    "\n",
    "With its exotic sounding name, a neural network model is often regarded as mysterious yet powerful predictive tool. Perhaps surprisingly, the most typical form of neural network, in fact, is a natural extension of regression model. This form of neural network is called **multilayer perceptron**, which is the subject of our practical today.\n",
    "\n",
    "<img src=\"resources/brain_vs_perceptron.png\" width=\"300\">\n",
    "*Brain neuron vs a perceptron*\n",
    "\n",
    "Whereas the strength of regression models is making decision in data with linear relationships, the strength of multi-layer perceptrons is their ability to go beyond linear relationships and model non-linear relationships in data.\n",
    "\n",
    "<img src=\"resources/mlp.png\" width=\"400\">\n",
    "*Multilayer perceptron structure*\n",
    "\n",
    "Multilayer perceptron models were originally inspired by structure and interconnections between neurons in brain. They are often represented using network diagram instead of an equation. The basic model form arranges neurons in layers. The first layer, called the **input layer**, connects to one or more **hidden layers**, which in turn, connect to the final layer called **target/output layer**. Connections between each layer correspond to certain set of weights, which like regression model, are optimised during training process.\n",
    "\n",
    "At the end of this practical, we would have built a number of predictive models. In practice, given a new dataset, data science professionals will build and experiment with many different models. Thus, it is important to understand how to compare these models and choose the best model. The second part of this practical guides you to assessing all of the models we have built so far - decision trees, logistic regressions and neural networks.\n",
    "\n",
    "In cases such as the financial and health domains, performance of a predictive model is crucial. To achieve even better performance, multiple models can be combined in together to achieve higher predictive performance than individual models. This approach is called **ensemble modeling** and it will be covered in the last part of this practical.\n",
    "\n",
    "## 1. Resuming from practical 4<a name=\"resume\"></a>\n",
    "Similar with practical 3 and 4, we will reuse the code for data preprocessing. Just as regression models, neural networks are sensitive to data on different scales, thus we will also perform standarization on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from dm_tools import data_prep\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# random state\n",
    "rs = 10\n",
    "\n",
    "# train test split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Building your first neural network model\n",
    "\n",
    "Start by importing your neural network from the library. In `sklearn`, neural network classifier is implemented in `MLPClassifier`, short for multilayer perceptron classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our first MLPClassifier. Initiate the model without any additional parameter (other than the random state for consistency), fit it to the training data and test its performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.882890855457\n",
      "Test accuracy: 0.546455609085\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.52      0.54      1453\n",
      "          1       0.54      0.57      0.56      1453\n",
      "\n",
      "avg / total       0.55      0.55      0.55      2906\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=10, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This default neural network performed alright, with 0.546 accuracy score on the test data. Similar with the first decision tree that we trained in practical 3, you should notice that the training accuracy is much higher than the test accuracy. This is an indication the model overfits to the training data, which we will fix through GridSearch tuning and dimensionality reduction techniques.\n",
    "\n",
    "### 2.1. Solver and activation function\n",
    "\n",
    "As discussed in the lecture, finding the best combination of weights in neural networks is a significant search problem. The algorithm used to find this optimal weight set is called **solver** in sklearn, and the most common one is gradient descent. Gradient descent starts with a set of randomly generated weights. In each iteration of gradient descent, predictions are made on X_train and the error value (cost) is computed. The weight set is then altered to reduce this error value. Each iteration is called an epoch. To stop gradient descent iterations, either one of **maximum iterations**, **minimum error threshold** or **convergence reached** (error is not improved over a certain number of epochs) strategies is used, with combination of max iterations and convergence being the most commonly used.\n",
    "\n",
    "In sklearn, if the neural network does not achieve convergence before maximum iteration, it will raise a \"convergence is not reached\" warning message. Fortunately, our first neural network reached convergence before maximum iteration limit (default to 200). If you see the message raised, the `max_iter` hyperparameter of the neural network should be increased. However, if even with very large number of maximum iteration the neural network still fails to reach convergence, this might indicate a problem with your error computation.\n",
    "\n",
    "The following code demonstrates the warning message raised by a MLP classifier with only 100 `max_iter`. Notice that the training accuracy is lower, as the solver has not finished optimising on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.82197640118\n",
      "Test accuracy: 0.55196145905\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.55      0.55      1453\n",
      "          1       0.55      0.55      0.55      1453\n",
      "\n",
      "avg / total       0.55      0.55      0.55      2906\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=10, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lieh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(max_iter=100, random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the MLPClassifier object hyperparameters printed out above. You should see that the `solver` hyperparameter is set by default to **adam** (stands for adaptive moment estimation). Adam is an extension of gradient descent, designed to speed up the training process and be more computationaly efficient. Adam is the solver algorithm of choice for many deep neural networks for its efficiency and we will use adam instead of normal gradient descent here.\n",
    "\n",
    "[A great explaination of adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "Another important hyperparameter to observe is `activation`, which refers to activation function used in hidden layers of the neural network. There are a number of options to use, including:\n",
    "* **identity**: $f(x) = x$, no transformation\n",
    "* **tanh**: $f(x) = {tanh}(x)$, used in lecture\n",
    "* **sigmoid**: $f(x) = \\frac{1}{1 + e^{h(\\theta, x)}}$, used in logistic regression\n",
    "* **relu - rectified linear unit**: $f(x) = max(x,0)$, default option in sklearn\n",
    "\n",
    "Identity function will change neural network into linear model, thus it is not commonly used. In the past, tanh and sigmoid are very popular, however, recent research suggested using **relu** in neural network can produce similarly accurate result at much lower training time. Therefore, we will use relu as our activation function in this practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding optimal hyperparameter with GridSearchCV\n",
    "\n",
    "Once we trained our first neural network, we will find the optimal hyperparameters using GridSearchCV. Neural network is harder to tune than decision trees or regression models due to relatively many type parameters and slow training process. In this practical, we will focus on tuning two parameters:\n",
    "1. `hidden_layer_sizes`: It has values of tuples, and within each tuple, element i-th represent the number of neurons contained in each hidden layer.\n",
    "2. `alpha`: L2 regularization parameter used in each neuron's activation function.\n",
    "\n",
    "Start by tuning the hidden layer sizes. There is no official guideline on how many neurons we should have in each layer, but for most data mining tasks a single hidden layer with neurons no more than the number of input variables and no less than output neurons (binary classification task, hence 1) is sufficient.\n",
    "\n",
    "> #### Deep Learning\n",
    "> You might have heard of deep learning, which is process of building very complex neural networks (up to hundreds of layers and thousands of neurons, hence **deep**). Deep neural networks are typically used for complex tasks, like image recognition, Siri-like voice assistant, machine translation and self-driving tasks.\n",
    "\n",
    "See how many input features we have by printing out the train shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6780, 85)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 85 features, we will start tuning with one hidden layer of 5 to 85 neurons, increment of 20. This might take a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.631120943953\n",
      "Test accuracy: 0.557123193393\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.57      0.56      1453\n",
      "          1       0.56      0.54      0.55      1453\n",
      "\n",
      "avg / total       0.56      0.56      0.56      2906\n",
      "\n",
      "{'hidden_layer_sizes': (5,)}\n"
     ]
    }
   ],
   "source": [
    "params = {'hidden_layer_sizes': [(x,) for x in range(5, 86, 20)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this GridSearchCV returns 5 neurons as the optimal number of neurons in the hidden layer. From the past practicals and this output, it seems like less complex models (smaller trees, smaller feature set) tend to generalise on this dataset. Based on this information, we should tune around the lower number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.605457227139\n",
      "Test accuracy: 0.557467309016\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.59      0.57      1453\n",
      "          1       0.56      0.53      0.54      1453\n",
      "\n",
      "avg / total       0.56      0.56      0.56      2906\n",
      "\n",
      "{'hidden_layer_sizes': (3,)}\n"
     ]
    }
   ],
   "source": [
    "# new parameters\n",
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the optimal value for neuron count in our hidden layer. Next, we will tune the second hyperparameter, which is `alpha`. The default value for `alpha` is `0.0001`, thus we will try `alpha` values around this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.606194690265\n",
      "Test accuracy: 0.558499655884\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.59      0.57      1453\n",
      "          1       0.56      0.53      0.54      1453\n",
      "\n",
      "avg / total       0.56      0.56      0.56      2906\n",
      "\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (3,)}\n"
     ]
    }
   ],
   "source": [
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch returned a hidden layer of 3 neurons and alpha value of 0.01 as the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality reduction\n",
    "\n",
    "In this section, we will try to improve performance of the model through dimensionality reduction and transformation techniques covered last week.\n",
    "\n",
    "### 5.1. Log transformation\n",
    "\n",
    "Similar with logistic regression, correcting skewed distributions in variables could benefit the performance of the neural network model. We will re-use the code from previous practical to perform log transformation on the skewed GiftAvg and GiftCnt variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# list columns to be transformed\n",
    "columns_to_transform = ['GiftAvg36', 'GiftAvgAll', 'GiftAvgCard36', 'GiftAvgLast',\n",
    "                        'GiftCnt36', 'GiftCntAll', 'GiftCntCard36', 'GiftCntCardAll']\n",
    "\n",
    "# copy the dataframe\n",
    "df_log = df.copy()\n",
    "\n",
    "# transform the columns with np.log\n",
    "for col in columns_to_transform:\n",
    "    df_log[col] = df_log[col].apply(lambda x: x+1)\n",
    "    df_log[col] = df_log[col].apply(np.log)\n",
    "    \n",
    "# create X, y and train test data partitions\n",
    "y_log = df_log['TargetB']\n",
    "X_log = df_log.drop(['TargetB'], axis=1)\n",
    "X_mat_log = X_log.as_matrix()\n",
    "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_mat_log, y_log, test_size=0.3, stratify=y_log, \n",
    "                                                                    random_state=rs)\n",
    "\n",
    "# standardise them again\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log = scaler_log.fit_transform(X_train_log, y_train_log)\n",
    "X_test_log = scaler_log.transform(X_test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is transformed, train and tune a MLPClassifier with GridSearchCV and see if it improves the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.615634218289\n",
      "Test accuracy: 0.568479008947\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.50      0.54      1453\n",
      "          1       0.56      0.64      0.60      1453\n",
      "\n",
      "avg / total       0.57      0.57      0.57      2906\n",
      "\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (5,)}\n"
     ]
    }
   ],
   "source": [
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_log, y_train_log)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_log, y_train_log))\n",
    "print(\"Test accuracy:\", cv.score(X_test_log, y_test_log))\n",
    "\n",
    "y_pred = cv.predict(X_test_log)\n",
    "print(classification_report(y_test_log, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With log transformation, the MLPClassifier sees improvement in both train and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Recursive Feature Elimination\n",
    "\n",
    "Next, we will try to reduce the feature set size using RFE. We will need a base elimination model and RFE requires type of model that assigns weight/feature importance to each feature (like regression/decision tree). Unfortunately, neural networks provide neither, thus we will try to use LogisticRegression as the base elimination model.\n",
    "\n",
    "As the log transformation has proven to improve model performance, we will use the transformed dataset instead of the original dataset for RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rfe = RFECV(estimator = LogisticRegression(random_state=rs), cv=10)\n",
    "rfe.fit(X_train_log, y_train_log)\n",
    "\n",
    "print(rfe.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFE with logistic regression has selected 34 features as the best set of features. With these selected features, tune an `MLPClassifier` with the transformed data set as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.59587020649\n",
      "Test accuracy: 0.56159669649\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.60      0.58      1453\n",
      "          1       0.57      0.52      0.54      1453\n",
      "\n",
      "avg / total       0.56      0.56      0.56      2906\n",
      "\n",
      "{'alpha': 1e-05, 'hidden_layer_sizes': (3,)}\n"
     ]
    }
   ],
   "source": [
    "# transform log \n",
    "X_train_rfe = rfe.transform(X_train_log)\n",
    "X_test_rfe = rfe.transform(X_test_log)\n",
    "\n",
    "# step = int((X_train_rfe.shape[1] + 5)/5);\n",
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_rfe, y_train_log)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_rfe, y_train_log))\n",
    "print(\"Test accuracy:\", cv.score(X_test_rfe, y_test_log))\n",
    "\n",
    "y_pred = cv.predict(X_test_rfe)\n",
    "print(classification_report(y_test_log, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFE selected feature set showed improvements over the original data set. However, compared with the previous best model (all features from log transformed dataset), the model performed slightly worse. This is an indication that elimination with logistic regression did not produce feature set suitable for neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Selecting using decision tree\n",
    "\n",
    "Lastly, we will use decision tree and feature importance produces from the model to perform feature selection. To start, we need to tune a decision tree with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=10,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'max_depth': range(3, 8), 'min_samples_leaf': range(20, 61, 10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(3, 8),\n",
    "          'min_samples_leaf': range(20, 61, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(X_train_log, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftCnt36 : 0.227725130555\n",
      "DemMedHomeValue : 0.151012685301\n",
      "GiftAvgLast : 0.109239815857\n",
      "GiftTimeLast : 0.0726940693811\n",
      "DemAge : 0.0505020293148\n",
      "GiftAvgCard36 : 0.0466386006863\n",
      "DemPctVeterans : 0.0398307738873\n",
      "GiftAvgAll : 0.0386717516844\n",
      "GiftTimeFirst : 0.0381328472113\n",
      "StatusCatStarAll : 0.0325337937087\n",
      "GiftCntAll : 0.0318417515854\n",
      "GiftCntCardAll : 0.0312619335355\n",
      "PromCnt36 : 0.0275876801696\n",
      "PromCntCardAll : 0.0269797831956\n",
      "PromCntAll : 0.0199307489149\n",
      "DemMedIncome : 0.0140041424717\n",
      "PromCnt12 : 0.0131861443574\n",
      "StatusCat96NK_A : 0.0118624603346\n",
      "PromCntCard36 : 0.0102226654036\n",
      "PromCntCard12 : 0.00614119244497\n"
     ]
    }
   ],
   "source": [
    "from dm_tools import analyse_feature_importance\n",
    "\n",
    "analyse_feature_importance(cv.best_estimator_, X_log.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6780, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selectmodel = SelectFromModel(cv.best_estimator_, prefit=True)\n",
    "X_train_sel_model = selectmodel.transform(X_train)\n",
    "X_test_sel_model = selectmodel.transform(X_test)\n",
    "\n",
    "print(X_train_sel_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SelectFromModel` reduces the log transformed dataset into only 18 variables. Proceed to tune a MLPClassifier with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.594690265487\n",
      "Test accuracy: 0.557811424639\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.61      0.58      1453\n",
      "          1       0.57      0.50      0.53      1453\n",
      "\n",
      "avg / total       0.56      0.56      0.56      2906\n",
      "\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (5,)}\n"
     ]
    }
   ],
   "source": [
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_sel_model, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_sel_model, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test_sel_model, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test_sel_model)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar with RFE in the previous step, the SelectFromModel with decision tree did not manage to improve model performance too. Therefore, we will keep the log transformed model as the best performing neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Models\n",
    "\n",
    "We have learned how to perform data preprocessing, building various predictive models, tuning them and applying dimensionality reduction techniques. In a data mining project, you typically need to test numerous combinations of models and data techniques before choosing a solution to deploy. To aid with deciding between multiple solutions, we will discuss some comparing measures.\n",
    "\n",
    "Firstly, train and tune three models of `DecisionTreeClassifier`, `LogisticRegression` and `MLPClassifier` with GridSearchCV. We will use the original feature set with no dimensionality reduction for this demonstration, but the process should not be too much different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=40, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=10,\n",
      "            splitter='best')\n",
      "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=10, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(3,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=10, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# grid search CV for decision tree\n",
    "params_dt = {'criterion': ['gini'],\n",
    "          'max_depth': range(2, 5),\n",
    "          'min_samples_leaf': range(40, 61, 5)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params_dt, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "dt_model = cv.best_estimator_\n",
    "print(dt_model)\n",
    "\n",
    "# grid search CV for logistic regression\n",
    "params_log_reg = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params_log_reg, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "log_reg_model = cv.best_estimator_\n",
    "print(log_reg_model)\n",
    "\n",
    "# grid search CV for NN\n",
    "params_nn = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [0.01,0.001, 0.0001, 0.00001]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params_nn, estimator=MLPClassifier(max_iter=500, random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "nn_model = cv.best_estimator_\n",
    "print(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Test Accuracy\n",
    "\n",
    "The first statistic we will use to compare these three models is accuracy. We have been using accuracy to perform model validation and tuning so far. Accuracy is a great statistics when the ratio of target classes are relatively equal, like what we have in this dataset (50% donors vs 50% non-donors). In cases where the targets are not equal (e.g. in cancer detection task where most people in the dataset will not have cancer), metrics like precision/recall/F1 from `classification_report` or Cohen's kappa are preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on test for DT: 0.546455609085\n",
      "Accuracy score on test for logistic regression: 0.565381968341\n",
      "Accuracy score on test for NN: 0.558499655884\n"
     ]
    }
   ],
   "source": [
    "y_pred_dt = dt_model.predict(X_test)\n",
    "y_pred_log_reg = log_reg_model.predict(X_test)\n",
    "y_pred_nn = nn_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score on test for DT:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Accuracy score on test for logistic regression:\", accuracy_score(y_test, y_pred_log_reg))\n",
    "print(\"Accuracy score on test for NN:\", accuracy_score(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test accuracy score, logistic regression performs the best, followed by decision tree and neural network.\n",
    "\n",
    "### 6.2. ROC AUC\n",
    "Another commonly used metric is receiver operating characteristic (ROC) and area under curve (AUC). ROC refers to the ability of binary classifier (like what we have here) to classify with varied discrimination threshold.\n",
    "\n",
    "Most predictive classification models produce probability of target values on a set of inputs. `LogisticRegression` and `MLPClassifier` produces real value probabilities, while `DecisionTree` has the ratio of majority classes in each leaf node. Most of the time, discrimination threshold is cap at 0.5, which means any probability prediction above 0.5 is considered as positive (and the rest negative). For more clarity, see this code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability produced by decision tree for each class vs actual prediction on TargetB (0 = non-donor, 1 = donor). You should be able to see the default threshold of 0.5.\n",
      "(Probs on zero)\t(probs on one)\t(prediction made)\n",
      "0.341238471673 \t 0.658761528327 \t 1\n",
      "0.468456375839 \t 0.531543624161 \t 1\n",
      "0.483870967742 \t 0.516129032258 \t 1\n",
      "0.483870967742 \t 0.516129032258 \t 1\n",
      "0.468456375839 \t 0.531543624161 \t 1\n",
      "0.483870967742 \t 0.516129032258 \t 1\n",
      "0.468456375839 \t 0.531543624161 \t 1\n",
      "0.468456375839 \t 0.531543624161 \t 1\n",
      "0.468456375839 \t 0.531543624161 \t 1\n",
      "0.646887159533 \t 0.353112840467 \t 0\n",
      "0.483870967742 \t 0.516129032258 \t 1\n",
      "0.646887159533 \t 0.353112840467 \t 0\n",
      "0.468456375839 \t 0.531543624161 \t 1\n",
      "0.257142857143 \t 0.742857142857 \t 1\n",
      "0.483870967742 \t 0.516129032258 \t 1\n",
      "0.483870967742 \t 0.516129032258 \t 1\n",
      "0.257142857143 \t 0.742857142857 \t 1\n",
      "0.483870967742 \t 0.516129032258 \t 1\n",
      "0.495652173913 \t 0.504347826087 \t 1\n",
      "0.341238471673 \t 0.658761528327 \t 1\n"
     ]
    }
   ],
   "source": [
    "# typical prediction\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# probability prediction from decision tree\n",
    "y_pred_proba_dt = dt_model.predict_proba(X_test)\n",
    "\n",
    "print(\"Probability produced by decision tree for each class vs actual prediction on TargetB (0 = non-donor, 1 = donor). You should be able to see the default threshold of 0.5.\")\n",
    "print(\"(Probs on zero)\\t(probs on one)\\t(prediction made)\")\n",
    "# print top 10\n",
    "for i in range(20):\n",
    "    print(y_pred_proba_dt[i][0], '\\t', y_pred_proba_dt[i][1], '\\t', y_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this concept in mind, ROC AUC score aims to find the best model under varied threshold. To compute our ROC AUC score, use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC index on test for DT: 0.575132305707\n",
      "ROC index on test for logistic regression: 0.592773145624\n",
      "ROC index on test for NN: 0.585721262083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_proba_dt = dt_model.predict_proba(X_test)\n",
    "y_pred_proba_log_reg = log_reg_model.predict_proba(X_test)\n",
    "y_pred_proba_nn = nn_model.predict_proba(X_test)\n",
    "\n",
    "roc_index_dt = roc_auc_score(y_test, y_pred_proba_dt[:, 1])\n",
    "roc_index_log_reg = roc_auc_score(y_test, y_pred_proba_log_reg[:, 1])\n",
    "roc_index_nn = roc_auc_score(y_test, y_pred_proba_nn[:, 1])\n",
    "\n",
    "print(\"ROC index on test for DT:\", roc_index_dt)\n",
    "print(\"ROC index on test for logistic regression:\", roc_index_log_reg)\n",
    "print(\"ROC index on test for NN:\", roc_index_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression` produces the best ROC score. This means on varied discrimination threshold, this LogReg model performs better compared to the other two models.\n",
    "\n",
    "ROC score only tells one side of the story, however. Typically, instead of ROC score, we plot a curve to show the performance of the model on different threshold values. The curve should look something like this, and the closer the curve is to top left corner, the better the model is.\n",
    "\n",
    "![ROC Curve](http://gim.unmc.edu/dxtests/roccomp.jpg)\n",
    "\n",
    "Let's plot ROC curve for our models. Firstly, we need to find the false positive rate, true positive rate and thresholds used for each models. We can get it from the `roc_curve` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_pred_proba_dt[:,1])\n",
    "fpr_log_reg, tpr_log_reg, thresholds_log_reg = roc_curve(y_test, y_pred_proba_log_reg[:,1])\n",
    "fpr_nn, tpr_nn, thresholds_nn = roc_curve(y_test, y_pred_proba_nn[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have these scores, plot them using `matplotlib`'s pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVMABhdtSkigAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd122128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(fpr_dt, tpr_dt, label='ROC Curve for DT {:.3f}'.format(roc_index_dt), color='red', lw=0.5)\n",
    "plt.plot(fpr_log_reg, tpr_log_reg, label='ROC Curve for Log reg {:.3f}'.format(roc_index_log_reg), color='green', lw=0.5)\n",
    "plt.plot(fpr_nn, tpr_nn, label='ROC Curve for NN {:.3f}'.format(roc_index_nn), color='darkorange', lw=0.5)\n",
    "\n",
    "# plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "#          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=0.5, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, you can see the curve for different models. `LogisticRegression` again has the largest curve area compared to the other two models. Thus, all three statistics that we used collectively agreed on `LogisticRegression` being the best performing model overall.\n",
    "\n",
    "While statistics are vital, in a real project, performance is not always the priority. Some of the other aspects used to consider a best model are:\n",
    "1. **Interpretability**: how well humans can understand decision making process in the model. Decision trees and regressions to some extent are excellent at this, while neural networks are much less interpretable.\n",
    "2. **Speed**: how well can the model train and predict on large amount of data. Again decision trees and regressions are relatively fast, while neural networks take a while to train.\n",
    "3. **Adaptability**: in cases where the model is deployed in a changing environment (e.g. predicting website user behaviour), the model needs to adapt to the changing data. Neural networks are great for this as they can be trained using \"online training\", while decision trees are not so great.\n",
    "\n",
    "## 7. Ensemble Modeling\n",
    "\n",
    "Ensemble modeling is a supervised, predictive modeling method that combines predictions from multiple models to produce a stronger model. Ensemble models are typically more accurate and more robust than the individual models they are built on. Typically, the individual models consist of different classes (e.g. combining decision tree and logistic regression) or they are trained on different subsets of the data (e.g. combining 2 decision trees, each trained on one half of training data).\n",
    "\n",
    "There are three major techniques in performing ensemble:\n",
    "1. **Bagging**: In bagging method, predictions from each models are combined through voting/averaging process. An example of bagging model is **Random Forest**, which is a bagging-based model consisted of many simple decision trees trained on random subsets of the data.\n",
    "2. **Boosting**: Boosting method combines models through sequential learning. The first model is learned and evaluated on the whole dataset. Each classes of misclassified training examples are then given higher priority/weight. Another model will be built to predict the misclassified classes correctly. This process will be continued until a set limit is reached (commonly number of models or accuracy improvement convergence). The most well known boosting models are **Gradient Boosting** and **AdaBoost**.\n",
    "3. **Stacking**: Stacking is similar to bagging. Instead of using simple voting/averaging process, stacking ensemble build another model to assign weights to predictions of each individual model.\n",
    "\n",
    "[A great introduction to ensemble learning and why it works very well](https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/)\n",
    "\n",
    "In this practical, we will build a simple voting-based bagging model to combine three models we built in section 6. Start by importing the `VotingClassifier` from `sklearn.ensemble`. After that, initialise the voting classifier as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# initialise the classifier with 3 different estimators\n",
    "voting = VotingClassifier(estimators=[('dt', dt_model), ('lr', log_reg_model), ('nn', nn_model)], voting='soft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two possible values for the `voting` hyperparameter, **hard voting** and **soft voting**. With hard voting, the predictions are made using the majority predicted value, while soft voting computes predictions based on softmax value of predicted probabilities. With well-calibrated models (we did calibration through GridSearchCV), `sklearn` recommends soft voting.\n",
    "\n",
    "Fit the voting model to the training data. After that, evaluate the accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble train accuracy: 0.605457227139\n",
      "Ensemble test accuracy: 0.567102546456\n",
      "ROC score of voting classifier: 0.595562068938\n"
     ]
    }
   ],
   "source": [
    "# fit the voting classifier to training data\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "# evaluate train and test accuracy\n",
    "print(\"Ensemble train accuracy:\", voting.score(X_train, y_train))\n",
    "print(\"Ensemble test accuracy:\", voting.score(X_test, y_test))\n",
    "\n",
    "# evaluate ROC auc score\n",
    "y_pred_proba_ensemble = voting.predict_proba(X_test)\n",
    "roc_index_ensemble = roc_auc_score(y_test, y_pred_proba_ensemble[:, 1])\n",
    "print(\"ROC score of voting classifier:\", roc_index_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble method managed to produce higher test accuracy and ROC score compared to the three individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Notes and Next Practical\n",
    "\n",
    "This practical, we learned how to build, tune and explore the structure of neural network models. We explored dimensionality reduction and transformation techniques to reduce the size of the feature set and improve performance of our neural network model. In addition, numerous statistics are used to compare end-to-end performance of all models we have built so far. Lastly, we built a voting, bagging-based ensemble model from three smaller models.\n",
    "\n",
    "Next week, we will have a drop-in help session where you focus on the assignment and ask your tutor questions regarding it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
